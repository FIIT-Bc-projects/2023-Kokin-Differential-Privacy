`% create gh repository (check)

% setup env (check);
% install 5 DP frameworks (opendp, opacus, tensorflow-privacy, diffprivlib, Tumult);
% get 2 - 3 datasets from Kaggle.com;
% preprocess data;
% tryout training a few models;

MAIN GOAL for 7th week
Train DNN with 1 DP framework and Hyperparameter visualization and follow dependencies
(Optional) Experiment with depth and width of DNN


04102023 Current TODOs
 - CoockieCutter - Done
 - 2 datasets EDA - Done, 
    - Select 1 dataset for further work
 - MNIST Classification with TF-Privacy
 - Hyperparameter tuning with Differential Privacy (epsilon as on of the inputs)

 11102023 Current State
Done
- CNN Model train with DP in TF.privacy with MNIST
- Dataset: https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease
TODOs
 - Figure out problem with batchsize/ input shape to train DNN on selected dataset - 
 - compare metrics in the graphs (relation with ACC, LOSS, Epsilon)
 - GRIDSEARCH -https://scikit-learn.org/stable/modules/grid_search.html
 - https://wandb.ai/site - set up for visualization

 18102023 Current State:
 Done 
 - Gridsearch implemented without dp
 - CNN Model train with DP in TF.privacy with MNIST
 - https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial_keras_model.py

TODOs
- compare metrics in the graphs (relation with ACC, LOSS, Epsilon)
- Wandb in progress


25102023 Current State:
Done:
 - Gridsearch implemented without DP
 - Wandb in progress
 - https://docs.wandb.ai/guides/integrations/tensorflow

TODOs
- Train model without the DP to estabish the baseline
- configurable epsilon prior to model training 
- compare metrics in the graphs (relation with ACC, LOSS, Epsilon)
- Notebook + TF + TF.Privacy -> Docker Image (future)


03112023 Current State:
Done:
 - GridSearch Partially working
 - Baseline - ok, val_acc 0.91 , lost 0.264
 - 
 TODOs
 - Save all the images. and training parameter. collect metrics
 - Try liner regression
 - MNIST for DNN to work on the benchmark
 - Prepare presentation of current state. 
 - Experiment with width of NN

 09112023 Current State:
 Done:
 - graphs done
 - variable noise multiplier
 - Linear Regression and DNN

 TODOs:
 - Collect core metrics for future big table
 - Describe correlation between Epsilon and Noise Multiplier (graph/table)
 - 2nd semester RNN
 - Try implement regular DNN but with different kind of DP optimizers https://github.com/tensorflow/privacy/blob/39c8a8c1af3b2bc499e9070ba36135d9a3f815cd/tensorflow_privacy/privacy/optimizers/dp_optimizer.py#L373 
 - compare different optimizers to baseline DNN and DNN+DP


 BP1 Structure
 - intro empty
 - SOTA - ML, Regression, Deep LEarning, DNN, Differential Privacy, DP Frameworks overview.
 - Objectives and methodology - empty
 - Experiments - Dataset, Baseline Exp, Current Case Studies/ Scenarios + Initial Results
 - conclusion - empty


15112023 Current State:
Done:
- Big table with metrics is in progress and almost finished
- Custom DP Optimizers and training cycle in progress

TODO:
- Smaller/Bigger batch size
- Start writing thesis

22112023 Current State:
TODO:
- add time measurement for model training in different setting
- produce cool graphs from data 


14122023 Current State:
Writing
- finish up with SOT for ML/DL
- Differntial Privacy (Local, Global, Renyi)
- DP frameworks (table comparison)
- Experiments
    - Context of experiment
    - Dataset
    - DNN architecture
    - experiment setup
    - preliminary results

- Annotation
- Objectives (3-4 points of main goals for thesis)
- Methodology (usually after sota and related work) (steps to solve objectives from above)




 

 
 
 
THOUGHTS:
Dynamic noise application, higher layer -> bigger noise.